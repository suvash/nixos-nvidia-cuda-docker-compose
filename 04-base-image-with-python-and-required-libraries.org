* 04. Prepare an image with Python & deep learning libraries you'll always need

It's a good idea to prepare a separate base image with the Python runtime and libraries you'll always need, specially the ones that are tied to a particular CUDA version.

When you have a separate base image, it's easier to test a new base image for CUDA, Python and required library updates before you swap out the image in your projects. This way it's safer to test out updates with minimum breakage.

There are various ways to go about this. If needed, the image can be built from scratch, but I've chosen to start out from an official cuda base image with ubuntu on it.

The files below have been tangled and made available in the 04-files directory. Feel free to copy over and continue directly.

** Copy over the contents of the 04-files  and update as needed

Copy the contents of 04-files to a new project directory. Initialize a git repo since the sha is automatically used in the tag.

Make sure that you're matching the CUDA version on your host operating system to the base image in the Dockerfile. Adjust the versions of python libraries as per the cuda requirements.

** Build the base image and test python cuda support

Adjust the content of Dockerfile and libraries as per your need.

Simply build the image afterwards. This will take a while. Afterwards you should be able to use this image as a base for most of your deep learning projects.

Make a note of the image tag as it'll be needed in next step.

#+begin_src shell
  cd /to/the/correct/folder/where/makefile/is/located
  make build

  # Test the image, get a terminal in the container and test cuda
  docker run --rm --gpus all -it suvash/deeplibs:py3.10-cuda12.2-ubuntu22.04-<INSERT-SHA-HERE> /bin/bash
  python -c "import torch; print(torch.cuda.is_available())"
  python -c "import jax; print(jax.local_devices())"
  exit

  # If you'd like to remove the image for some reason
  make clean
#+end_src

